name: Rewardsy Infrastructure Health Monitoring

on:
  schedule:
    # Every 10 minutes for continuous monitoring
    - cron: '*/10 * * * *'
    # Daily summary at 9 AM IST (3:30 AM UTC)
    - cron: '30 3 * * *'
  workflow_dispatch:
    inputs:
      check_type:
        description: 'Type of health check to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - kubernetes
          - applications
          - supporting-services

env:
  # ========================================
  # DISCORD CONFIGURATION
  # ========================================
  DISCORD_USER_MENTION: "<@1379840575783829544>"
  DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK_URL }}
  
  # ========================================
  # AZURE AKS CONFIGURATION
  # ========================================
  AKS_CLUSTER_NAME: rewardsy-prod-aks
  AKS_RESOURCE_GROUP: rewardsy-prod-rg
  
  # ========================================
  # HEALTH CHECK CONFIGURATION
  # ========================================
  REQUEST_TIMEOUT: 30
  ACCEPTABLE_RESPONSE_CODES: "200,302,404"
  CPU_THRESHOLD: 80
  MEMORY_THRESHOLD: 85
  
  # ========================================
  # APPLICATION ENDPOINTS (Primary - via Ingress)
  # ========================================
  ENDPOINTS: |
    {
      "backend": {
        "name": "Backend API",
        "primary": "https://app.rewardsy.one/rewardsy-backend/docs/core",
        "fallback": "",
        "expected_codes": [200],
        "critical": true
      },
      "webapp": {
        "name": "Web Application",
        "primary": "https://app.rewardsy.one",
        "fallback": "http://4.186.15.176",
        "expected_codes": [200, 302],
        "critical": true
      },
      "merchant": {
        "name": "Merchant Portal",
        "primary": "https://merchant.rewardsy.one",
        "fallback": "http://135.235.254.253",
        "expected_codes": [200, 302, 404],
        "critical": true
      },
      "data": {
        "name": "Data Service",
        "primary": "https://app.rewardsy.one/rewardsy-data/health",
        "fallback": "http://4.213.202.43/health",
        "expected_codes": [200, 404],
        "critical": true
      },
      "suprpixl": {
        "name": "SuprPixl Service",
        "primary": "https://suprpixl.rewardsy.one",
        "fallback": "http://4.187.129.223",
        "expected_codes": [200, 302, 404, 301],
        "critical": false
      },
      "kibana": {
        "name": "Kibana Dashboard",
        "primary": "https://app.rewardsy.one/kibana/app/home",
        "fallback": "http://98.70.242.255",
        "expected_codes": [200, 302],
        "critical": false
      },
      "kafka_ui": {
        "name": "Kafka UI",
        "primary": "https://app.rewardsy.one/kafka-ui",
        "fallback": "",
        "expected_codes": [200],
        "critical": false
      },
      "ingress": {
        "name": "Ingress NGINX",
        "primary": "http://52.140.85.8/healthz",
        "fallback": "",
        "expected_codes": [200, 404],
        "critical": true
      }
    }

jobs:
  # ================================================================
  # KUBERNETES INFRASTRUCTURE HEALTH
  # ================================================================
  
  kubernetes-health:
    name: Kubernetes Cluster Health
    runs-on: ubuntu-latest
    if: |
      github.event_name != 'workflow_dispatch' || 
      github.event.inputs.check_type == 'all' || 
      github.event.inputs.check_type == 'kubernetes'
    
    outputs:
      cluster_healthy: ${{ steps.aggregate.outputs.cluster_healthy }}
      node_status: ${{ steps.nodes.outputs.status }}
      node_count: ${{ steps.nodes.outputs.node_count }}
      notready_nodes: ${{ steps.nodes.outputs.notready_nodes }}
      avg_cpu: ${{ steps.nodes.outputs.avg_cpu }}
      avg_memory: ${{ steps.nodes.outputs.avg_memory }}
      pod_status: ${{ steps.pods.outputs.status }}
      total_pods: ${{ steps.pods.outputs.total_pods }}
      problem_count: ${{ steps.pods.outputs.problem_count }}
      problematic_pods: ${{ steps.pods.outputs.problematic_pods }}
      high_cpu_nodes: ${{ steps.nodes.outputs.high_cpu_nodes }}
      high_memory_nodes: ${{ steps.nodes.outputs.high_memory_nodes }}
    
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Set AKS Context
        uses: azure/aks-set-context@v3
        with:
          cluster-name: ${{ env.AKS_CLUSTER_NAME }}
          resource-group: ${{ env.AKS_RESOURCE_GROUP }}
      
      - name: Check Node Health & Resource Utilization
        id: nodes
        continue-on-error: true
        run: |
          echo "üîç Checking AKS node health and utilization..."
          
          # Get node status
          notready_nodes=$(kubectl get nodes --no-headers | grep -v " Ready " | awk '{print $1}' | tr '\n' ',' || echo "")
          total_nodes=$(kubectl get nodes --no-headers | wc -l)
          ready_nodes=$(kubectl get nodes --no-headers | grep " Ready " | wc -l)
          
          echo "node_count=$total_nodes" >> $GITHUB_OUTPUT
          echo "ready_nodes=$ready_nodes" >> $GITHUB_OUTPUT
          
          # Check node metrics
          if kubectl top nodes &>/dev/null; then
            node_metrics=$(kubectl top nodes --no-headers)
            
            # Calculate averages
            avg_cpu=$(echo "$node_metrics" | awk '{gsub(/%/,"",$3); sum+=$3} END {printf "%.0f", sum/NR}')
            avg_memory=$(echo "$node_metrics" | awk '{gsub(/%/,"",$5); sum+=$5} END {printf "%.0f", sum/NR}')
            
            echo "avg_cpu=${avg_cpu}%" >> $GITHUB_OUTPUT
            echo "avg_memory=${avg_memory}%" >> $GITHUB_OUTPUT
            
            # Check for nodes exceeding thresholds
            high_cpu_nodes=$(echo "$node_metrics" | awk -v threshold=${{ env.CPU_THRESHOLD }} '{gsub(/%/,"",$3); if($3>threshold) print $1": "$3"%"}' | tr '\n' ',' || echo "")
            high_memory_nodes=$(echo "$node_metrics" | awk -v threshold=${{ env.MEMORY_THRESHOLD }} '{gsub(/%/,"",$5); if($5>threshold) print $1": "$5"%"}' | tr '\n' ',' || echo "")
            
            echo "high_cpu_nodes=${high_cpu_nodes}" >> $GITHUB_OUTPUT
            echo "high_memory_nodes=${high_memory_nodes}" >> $GITHUB_OUTPUT
            
            # Warn if thresholds exceeded
            if [ -n "$high_cpu_nodes" ] || [ -n "$high_memory_nodes" ]; then
              echo "‚ö†Ô∏è Resource utilization exceeds thresholds!"
              [ -n "$high_cpu_nodes" ] && echo "High CPU nodes: $high_cpu_nodes"
              [ -n "$high_memory_nodes" ] && echo "High Memory nodes: $high_memory_nodes"
            fi
          else
            echo "avg_cpu=N/A" >> $GITHUB_OUTPUT
            echo "avg_memory=N/A" >> $GITHUB_OUTPUT
            echo "high_cpu_nodes=" >> $GITHUB_OUTPUT
            echo "high_memory_nodes=" >> $GITHUB_OUTPUT
          fi
          
          # Set outputs for NotReady nodes
          if [ -n "$notready_nodes" ]; then
            echo "notready_nodes=${notready_nodes%,}" >> $GITHUB_OUTPUT
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "‚ùå NotReady nodes detected: ${notready_nodes%,}"
            exit 1
          else
            echo "notready_nodes=None" >> $GITHUB_OUTPUT
            echo "status=success" >> $GITHUB_OUTPUT
            echo "‚úÖ All $total_nodes nodes are Ready (CPU: ${avg_cpu}%, Memory: ${avg_memory}%)"
          fi
      
      - name: Check Pod Health Across Namespaces
        id: pods
        continue-on-error: true
        run: |
          echo "üîç Checking pod health in application namespaces..."
          
          # Define namespaces to monitor
          NAMESPACES=(
            "rewardsy-backend"
            "rewardsy-webapp"
            "rewardsy-data"
            "rewardsy-merchant"
            "rewardsy-suprpixl"
            "kafka"
            "ingress-nginx"
          )
          
          problematic_pods=""
          total_pods=0
          problem_count=0
          
          for ns in "${NAMESPACES[@]}"; do
            # Check if namespace exists
            if ! kubectl get namespace "$ns" &>/dev/null; then
              echo "‚ö†Ô∏è Namespace '$ns' not found, skipping..."
              continue
            fi
            
            # Count pods
            ns_pod_count=$(kubectl get pods -n "$ns" --no-headers 2>/dev/null | wc -l || echo "0")
            total_pods=$((total_pods + ns_pod_count))
            
            # Find problematic pods
            non_running=$(kubectl get pods -n "$ns" \
              --field-selector=status.phase!=Running,status.phase!=Succeeded \
              --no-headers 2>/dev/null || echo "")
            
            if [ -n "$non_running" ]; then
              while IFS= read -r line; do
                pod_name=$(echo "$line" | awk '{print $1}')
                status=$(echo "$line" | awk '{print $3}')
                restarts=$(echo "$line" | awk '{print $4}')
                
                problematic_pods="${problematic_pods}${ns}/${pod_name} [${status}] (Restarts: ${restarts})"$'\n'
                problem_count=$((problem_count + 1))
              done <<< "$non_running"
            fi
          done
          
          # Set outputs
          echo "total_pods=$total_pods" >> $GITHUB_OUTPUT
          echo "problem_count=$problem_count" >> $GITHUB_OUTPUT
          
          if [ -n "$problematic_pods" ]; then
            {
              echo "problematic_pods<<EOF"
              echo "$problematic_pods"
              echo "EOF"
            } >> $GITHUB_OUTPUT
          else
            echo "problematic_pods=None" >> $GITHUB_OUTPUT
          fi
          
          # Set status
          if [ $problem_count -gt 0 ]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "‚ùå Found $problem_count problematic pods out of $total_pods total"
            exit 1
          else
            echo "status=success" >> $GITHUB_OUTPUT
            echo "‚úÖ All $total_pods application pods are healthy"
          fi
      
      - name: Aggregate Cluster Health
        id: aggregate
        if: always()
        run: |
          if [[ "${{ steps.nodes.outputs.status }}" == "success" ]] && \
             [[ "${{ steps.pods.outputs.status }}" == "success" ]]; then
            echo "cluster_healthy=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Kubernetes cluster is healthy"
          else
            echo "cluster_healthy=false" >> $GITHUB_OUTPUT
            echo "‚ùå Kubernetes cluster has issues"
          fi

  # ================================================================
  # APPLICATION & SUPPORTING SERVICES HEALTH
  # ================================================================
  
  service-health-checks:
    name: Service Health Checks
    runs-on: ubuntu-latest
    if: |
      github.event_name != 'workflow_dispatch' || 
      github.event.inputs.check_type == 'all' || 
      github.event.inputs.check_type == 'applications' ||
      github.event.inputs.check_type == 'supporting-services'
    
    outputs:
      results: ${{ steps.check_services.outputs.results }}
      failed_count: ${{ steps.check_services.outputs.failed_count }}
      critical_failures: ${{ steps.check_services.outputs.critical_failures }}
    
    steps:
      - name: Check All Service Endpoints
        id: check_services
        run: |
          echo "üîç Checking all service endpoints..."
          
          # Parse endpoints from env
          endpoints='${{ env.ENDPOINTS }}'
          
          results=""
          failed_count=0
          critical_failures=0
          
          # Function to check a single endpoint
          check_endpoint() {
            local service_key=$1
            local service_data=$2
            
            local name=$(echo "$service_data" | jq -r '.name')
            local primary=$(echo "$service_data" | jq -r '.primary')
            local fallback=$(echo "$service_data" | jq -r '.fallback')
            local expected_codes=$(echo "$service_data" | jq -r '.expected_codes[]' | tr '\n' ',' | sed 's/,$//')
            local is_critical=$(echo "$service_data" | jq -r '.critical')
            
            echo "Checking $name..."
            
            # Try primary endpoint
            start_time=$(date +%s%N)
            response=$(curl -s -o /dev/null -w "%{http_code}" --max-time ${{ env.REQUEST_TIMEOUT }} -k "$primary" 2>/dev/null || echo "000")
            end_time=$(date +%s%N)
            response_time=$(( (end_time - start_time) / 1000000 ))
            
            # If primary fails and fallback exists, try fallback
            if [[ "$response" == "000" || "$response" == "Connection refused" ]] && [ -n "$fallback" ] && [ "$fallback" != "null" ]; then
              echo "  Primary failed, trying fallback..."
              start_time=$(date +%s%N)
              response=$(curl -s -o /dev/null -w "%{http_code}" --max-time ${{ env.REQUEST_TIMEOUT }} "$fallback" 2>/dev/null || echo "000")
              end_time=$(date +%s%N)
              response_time=$(( (end_time - start_time) / 1000000 ))
            fi
            
            # Check if response code is acceptable
            local status="success"
            if [[ ",$expected_codes," == *",$response,"* ]]; then
              echo "  ‚úÖ $name: HTTP $response (${response_time}ms)"
            else
              status="failure"
              ((failed_count++))
              if [ "$is_critical" == "true" ]; then
                ((critical_failures++))
              fi
              echo "  ‚ùå $name: HTTP $response (Expected: $expected_codes)"
            fi
            
            # Append to results
            results="${results}${service_key}|${name}|${status}|${response}|${response_time}|${is_critical}"$'\n'
          }
          
          # Iterate through all endpoints
          for key in $(echo "$endpoints" | jq -r 'keys[]'); do
            service_data=$(echo "$endpoints" | jq -r --arg key "$key" '.[$key]')
            check_endpoint "$key" "$service_data"
          done
          
          # Set outputs
          {
            echo "results<<EOF"
            echo "$results"
            echo "EOF"
          } >> $GITHUB_OUTPUT
          
          echo "failed_count=$failed_count" >> $GITHUB_OUTPUT
          echo "critical_failures=$critical_failures" >> $GITHUB_OUTPUT
          
          echo ""
          echo "üìä Summary: $failed_count failures detected ($critical_failures critical)"

  # ================================================================
  # AGGREGATE RESULTS & SEND NOTIFICATIONS
  # ================================================================
  
  aggregate-and-notify:
    name: Aggregate & Notify
    runs-on: ubuntu-latest
    needs: 
      - kubernetes-health
      - service-health-checks
    if: always()
    
    steps:
      - name: Calculate Overall Health Status
        id: summary
        run: |
          echo "üìä Calculating overall health status..."
          
          # Initialize counters
          total_checks=0
          failed_checks=0
          critical_issues=0
          
          # Check Kubernetes health
          total_checks=$((total_checks + 2))  # nodes + pods
          
          if [[ "${{ needs.kubernetes-health.outputs.node_status }}" == "failure" ]]; then
            ((failed_checks++))
            ((critical_issues++))
          fi
          
          if [[ "${{ needs.kubernetes-health.outputs.pod_status }}" == "failure" ]]; then
            ((failed_checks++))
            critical_issues=$((critical_issues + ${{ needs.kubernetes-health.outputs.problem_count || 0 }}))
          fi
          
          # Add service check results
          service_failed=${{ needs.service-health-checks.outputs.failed_count || 0 }}
          service_critical=${{ needs.service-health-checks.outputs.critical_failures || 0 }}
          
          total_checks=$((total_checks + 8))  # 8 services
          failed_checks=$((failed_checks + service_failed))
          critical_issues=$((critical_issues + service_critical))
          
          # Determine severity
          if [ $failed_checks -eq 0 ]; then
            severity="healthy"
            color="3066993"
            emoji="‚úÖ"
          elif [ $critical_issues -gt 0 ]; then
            severity="critical"
            color="15158332"
            emoji="üî¥"
          else
            severity="warning"
            color="16776960"
            emoji="‚ö†Ô∏è"
          fi
          
          echo "total_checks=$total_checks" >> $GITHUB_OUTPUT
          echo "failed_checks=$failed_checks" >> $GITHUB_OUTPUT
          echo "critical_issues=$critical_issues" >> $GITHUB_OUTPUT
          echo "severity=$severity" >> $GITHUB_OUTPUT
          echo "color=$color" >> $GITHUB_OUTPUT
          echo "emoji=$emoji" >> $GITHUB_OUTPUT
          
          echo ""
          echo "Results: $failed_checks/$total_checks checks failed ($critical_issues critical)"
          echo "Severity: $severity"
      
      - name: Format Service Status
        id: format
        if: always()
        run: |
          # Parse service health results
          results='${{ needs.service-health-checks.outputs.results }}'
          
          backend_status="‚úÖ"
          webapp_status="‚úÖ"
          merchant_status="‚úÖ"
          data_status="‚úÖ"
          suprpixl_status="‚úÖ"
          kibana_status="‚úÖ"
          kafka_status="‚úÖ"
          ingress_status="‚úÖ"
          
          while IFS='|' read -r key name status response response_time critical; do
            [ -z "$key" ] && continue
            
            icon="‚úÖ"
            [ "$status" == "failure" ] && icon="üî¥"
            
            case "$key" in
              backend) backend_status="$icon Backend - [$response] (${response_time}ms)" ;;
              webapp) webapp_status="$icon WebApp - [$response] (${response_time}ms)" ;;
              merchant) merchant_status="$icon Merchant - [$response] (${response_time}ms)" ;;
              data) data_status="$icon Data Service - [$response]" ;;
              suprpixl) suprpixl_status="$icon SuprPixl - [$response] (${response_time}ms)" ;;
              kibana) kibana_status="$icon Kibana - [$response]" ;;
              kafka_ui) kafka_status="$icon Kafka UI - [$response]" ;;
              ingress) ingress_status="$icon Ingress NGINX" ;;
            esac
          done <<< "$results"
          
          echo "backend_status=$backend_status" >> $GITHUB_OUTPUT
          echo "webapp_status=$webapp_status" >> $GITHUB_OUTPUT
          echo "merchant_status=$merchant_status" >> $GITHUB_OUTPUT
          echo "data_status=$data_status" >> $GITHUB_OUTPUT
          echo "suprpixl_status=$suprpixl_status" >> $GITHUB_OUTPUT
          echo "kibana_status=$kibana_status" >> $GITHUB_OUTPUT
          echo "kafka_status=$kafka_status" >> $GITHUB_OUTPUT
          echo "ingress_status=$ingress_status" >> $GITHUB_OUTPUT
      
      - name: Send Alert Notification (Failures Only)
        if: steps.summary.outputs.failed_checks != '0'
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          nodetail: true
          title: "${{ steps.summary.outputs.emoji }} HEALTH ALERT - ${{ steps.summary.outputs.failed_checks }}/${{ steps.summary.outputs.total_checks }} Checks Failed"
          description: |
            ${{ env.DISCORD_USER_MENTION }} **IMMEDIATE ATTENTION REQUIRED**
            
            **Severity:** `${{ steps.summary.outputs.severity }}` | **Critical Issues:** ${{ steps.summary.outputs.critical_issues }}
            
            **üèóÔ∏è Infrastructure Health:**
            ${{ needs.kubernetes-health.outputs.node_status == 'failure' && 'üî¥' || '‚úÖ' }} AKS Nodes - ${{ needs.kubernetes-health.outputs.node_count || 'N/A' }} nodes (CPU: ${{ needs.kubernetes-health.outputs.avg_cpu || 'N/A' }}, Memory: ${{ needs.kubernetes-health.outputs.avg_memory || 'N/A' }})
            ${{ needs.kubernetes-health.outputs.pod_status == 'failure' && 'üî¥' || '‚úÖ' }} Kubernetes Pods - ${{ needs.kubernetes-health.outputs.total_pods || 'N/A' }} total
            
            ${{ needs.kubernetes-health.outputs.notready_nodes != 'None' && needs.kubernetes-health.outputs.notready_nodes != '' && format('‚ö†Ô∏è **NotReady Nodes:** {0}', needs.kubernetes-health.outputs.notready_nodes) || '' }}
            ${{ needs.kubernetes-health.outputs.high_cpu_nodes != '' && format('‚ö†Ô∏è **High CPU Nodes:** {0}', needs.kubernetes-health.outputs.high_cpu_nodes) || '' }}
            ${{ needs.kubernetes-health.outputs.high_memory_nodes != '' && format('‚ö†Ô∏è **High Memory Nodes:** {0}', needs.kubernetes-health.outputs.high_memory_nodes) || '' }}
            
            **üöÄ Application Services:**
            ${{ steps.format.outputs.backend_status }}
            ${{ steps.format.outputs.webapp_status }}
            ${{ steps.format.outputs.merchant_status }}
            ${{ steps.format.outputs.data_status }}
            ${{ steps.format.outputs.suprpixl_status }}
            
            **üîß Supporting Services:**
            ${{ steps.format.outputs.kibana_status }}
            ${{ steps.format.outputs.kafka_status }}
            ${{ steps.format.outputs.ingress_status }}
            
            ${{ needs.kubernetes-health.outputs.problem_count != '0' && needs.kubernetes-health.outputs.problem_count != '' && format('**‚ö†Ô∏è Problematic Pods ({0}):**\n```\n{1}\n```', needs.kubernetes-health.outputs.problem_count, needs.kubernetes-health.outputs.problematic_pods) || '' }}
            
            **üîó Quick Actions:**
            ‚Ä¢ [View Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            ‚Ä¢ [Azure Portal - AKS](https://portal.azure.com/#@rewardsy.com/resource/subscriptions/4b9e0373-83d3-42e8-95f9-be6625019af0/resourceGroups/rewardsy-prod-rg/providers/Microsoft.ContainerService/managedClusters/rewardsy-prod-aks/workloads)
            ‚Ä¢ [Kibana Dashboard](https://app.rewardsy.one/kibana)
            
            **üìÖ Timestamp:** ${{ github.event.repository.updated_at }}
          color: ${{ steps.summary.outputs.color }}
          username: Rewardsy Health Monitor
          avatar_url: https://cdn-icons-png.flaticon.com/512/2917/2917995.png
      
      - name: Send Daily Health Summary
        if: |
          steps.summary.outputs.failed_checks == '0' && 
          github.event.schedule == '30 3 * * *'
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          nodetail: true
          title: "‚úÖ Daily Health Report - All Systems Operational"
          description: |
            **üìä Rewardsy Production Environment Status**
            
            **Infrastructure Health:**
            ‚úÖ AKS Cluster: ${{ needs.kubernetes-health.outputs.node_count || 'N/A' }} nodes healthy
            ‚Ä¢ CPU Utilization: ${{ needs.kubernetes-health.outputs.avg_cpu || 'N/A' }}
            ‚Ä¢ Memory Utilization: ${{ needs.kubernetes-health.outputs.avg_memory || 'N/A' }}
            ‚Ä¢ Application Pods: ${{ needs.kubernetes-health.outputs.total_pods || 'N/A' }} running
            
            **Application Services:**
            ${{ steps.format.outputs.backend_status }}
            ${{ steps.format.outputs.webapp_status }}
            ${{ steps.format.outputs.merchant_status }}
            ${{ steps.format.outputs.data_status }}
            ${{ steps.format.outputs.suprpixl_status }}
            
            **Supporting Services:**
            ${{ steps.format.outputs.kibana_status }}
            ${{ steps.format.outputs.kafka_status }}
            ${{ steps.format.outputs.ingress_status }}
            
            **üìà Performance:**
            ‚Ä¢ All endpoints responding within acceptable latency
            ‚Ä¢ No resource utilization alerts
            ‚Ä¢ Zero problematic pods detected
            
            *Next health check in 10 minutes*
            *Daily report scheduled for 9:00 AM IST tomorrow*
          color: 3066993
          username: Rewardsy Health Monitor
          avatar_url: https://cdn-icons-png.flaticon.com/512/2917/2917995.png
      
      - name: Fail Workflow on Critical Issues
        if: steps.summary.outputs.critical_issues != '0'
        run: |
          echo "‚ùå Workflow failed due to ${{ steps.summary.outputs.critical_issues }} critical issues"
          exit 1
